{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import enchant\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input format for raw data should be tab-separated file with 2 columns\n",
    "- Timestamp (header == 'SubmissionDate')\n",
    "- Email thread text (header == Emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get all email data\n",
    "df = pd.read_table(\"./raw_data_tfidf.txt\")\n",
    "df.SubmissionDate = pd.to_datetime(df.SubmissionDate)\n",
    "df.index = df.SubmissionDate\n",
    "\n",
    "## group email texts per month\n",
    "dfm = pd.DataFrame()\n",
    "dfm['Emails'] = df.groupby(pd.Grouper(freq=\"M\"))['Emails'].apply(\"; \".join)\n",
    "\n",
    "# save month matrix\n",
    "dfm.to_csv('./raw_data_tfidf_month.txt', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_words = []\n",
    "d = enchant.Dict(\"en_US\")\n",
    "def cleanup_text_first_pass(df):\n",
    "    email_texts = TextBlob(df['Emails'])\n",
    "    to_keep = []\n",
    "    to_rm = []\n",
    "    to_check = []\n",
    "    for x in email_texts.words:\n",
    "        flag = True\n",
    "        not_in_d = False\n",
    "        # remove stop words\n",
    "        if x.lower() in stopwords.words('english'):\n",
    "            flag = False\n",
    "        # remove numbers\n",
    "        if x.strip().isdigit():\n",
    "            flag = False\n",
    "        # remove non english words\n",
    "        if not d.check(x.strip().lower()):\n",
    "            flag = False\n",
    "            not_in_d = True\n",
    "        if flag:\n",
    "            to_keep.append(x)\n",
    "        elif not_in_d:\n",
    "             to_check.append(x.lower())\n",
    "        else:\n",
    "            to_rm.append(x)\n",
    "    df['processedEmails'] = ' '.join(to_keep)\n",
    "    df['filteredwords'] = ' '.join(to_rm)\n",
    "    df['tocheck'] = ' '.join(to_check)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process email threads with above function.\n",
    "dfm_preprocessed = dfm.apply(cleanup_text_first_pass, axis=1)\n",
    "dfm_preprocessed.to_csv(\"./tf_idf_month_preprocessedtext.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going further, we need to double-check some of the token as acronyms and other actual words might have been filtered out. This process can be automated somewhat by looking at the frequency of excluded tokens overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get data for tokens to double check:\n",
    "all_to_check = ' '.join(dfm_preprocessed.tocheck)\n",
    "all_words_to_check = all_to_check.split(' ')\n",
    "check_words = TextBlob(' '.join(dfm_preprocessed.tocheck))\n",
    "tot_check = len(all_to_check.split(' '))\n",
    "print(\"Total number of tokens: \", tot_check)\n",
    "print(\"Number of unique tokens: \", len(set(all_words_to_check)))\n",
    "#34170\n",
    "#179460\n",
    "\n",
    "ct_words = {}\n",
    "freq_words = {}\n",
    "for i in range(0, tot_check):\n",
    "    x = all_words[i]\n",
    "    if x.strip():\n",
    "        if x in ct_words:\n",
    "            ct_words[x] = ct_words[x] + 1\n",
    "        else:\n",
    "            ct_words[x] = 1\n",
    "\n",
    "print(\"count done --\", len(ct_words), \" (should be number of unique tokens).\")\n",
    "\n",
    "for uw in ct_words:\n",
    "    freq_words[uw] = ct_words[uw] / tot_check\n",
    "\n",
    "# sort tokens by frequency, store in new DF\n",
    "sorted_freq_words = sorted(freq_words.items(), key=lambda t: t[1], reverse=True)\n",
    "sorteddf = pd.DataFrame(sorted_freq_words)\n",
    "sorteddf.columns = ['token', 'freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at frequency distribution\n",
    "plt.hist(sorteddf.freq, bins=100)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decide on a threshold based on distribution -- we double-checked tokens appearing > 2 times\n",
    "threshold_freq = 0.0001\n",
    "over_couple_time_tokens = sorteddf[sorteddf.freq > threshold_freq]\n",
    "over_couple_time_tokens.shape\n",
    "over_couple_time_tokens.to_csv(\"words_to_check.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "words_to_check.tsv contains the tokens that need to be visually inspected. Tokens to ultimately re-introduced can be saved in \"ok_words.txt\"  to be read in and included in the tokens to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ok_words = []\n",
    "with open('./ok_words.txt', 'r') as ow:\n",
    "    for l in ow:\n",
    "        ok_words.append(l.strip())\n",
    "\n",
    "def cleanup_text(df):\n",
    "    # to do the initial splitting/lowercasing etc\n",
    "    email_threads = TextBlob(df['Emails'])\n",
    "    to_keep = []\n",
    "    to_rm = []\n",
    "    to_check = []\n",
    "    all_the_words = []\n",
    "    for x in email_threads.words:\n",
    "        # split on '.' and strip apostrophes\n",
    "        tmplist = x.strip(\"'\").split(\".\")\n",
    "        for w in tmplist:\n",
    "            if w.strip():\n",
    "                all_the_words.append(w.lower())\n",
    "                flag = True\n",
    "                not_in_d = False\n",
    "                # remove stop words\n",
    "                if w.lower() in stopwords.words('english'):\n",
    "                    flag = False\n",
    "                # remove numbers\n",
    "                if w.strip().isdigit():\n",
    "                    flag = False\n",
    "                # remove non english words\n",
    "                if not d.check(w.strip().lower()):\n",
    "                    flag = False\n",
    "                    not_in_d = True\n",
    "                if flag:\n",
    "                    to_keep.append(w)\n",
    "                elif not_in_d:\n",
    "                    if w.lower() in ok_words:\n",
    "                        to_keep.append(w)\n",
    "                    else :\n",
    "                        to_check.append(w.lower())\n",
    "                else:\n",
    "                    to_rm.append(w)\n",
    "    df['allwords'] = ' '.join(all_the_words)\n",
    "    df['processedEmails'] = ' '.join(to_keep)\n",
    "    df['filteredwords'] = ' '.join(to_rm)\n",
    "    df['tocheck'] = ' '.join(to_check)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## process email threads\n",
    "dfmprocessed = dfm.apply(cleanup_text, axis=1)\n",
    "dfmprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalwords = ' '.join(dfmprocessed.allwords)\n",
    "all_ze_words = totalwords.split(' ')\n",
    "totw_check = len(all_ze_words)\n",
    "print(totw_check)\n",
    "print(len(set(all_ze_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF analysis\n",
    "tfidfm = TfidfVectorizer()\n",
    "corpus_month = [x for x in dfmprocessed.processedEmails]\n",
    "resp_m = tfidfm.fit_transform(corpus_month)\n",
    "feature_names_mth = tfidfm.get_feature_names()\n",
    "\n",
    "#per month\n",
    "monthly_words = []\n",
    "dense_month = resp_m.todense()\n",
    "monthly_top20 = []\n",
    "for mh in range(0, resp_m.shape[0]):\n",
    "    month_data = dense_month[mh].tolist()[0]\n",
    "    tot_data = len(month_data)\n",
    "    score_month = [x for x in zip(range(0, tot_data), month_data) if x[1] > 0]\n",
    "    rs_scores = sorted(score_month, key=lambda t: t[1] * -1)[:20]\n",
    "    top20 = []\n",
    "    for phrase, score in [(feature_names_mth[word_id], score) for (word_id, score) in rs_scores][:20]:\n",
    "        top20.append(phrase + \":\" + str(score))    \n",
    "        monthly_words.append(phrase)\n",
    "    monthly_top20.append(\";\".join(top20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmprocessed['top20'] = monthly_top20\n",
    "dfmprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmprocessed[['top20']].to_csv(\"./tfidf_month_processedtext_top20.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look at TF-IDF for specific words apperaing in top20:\n",
    "words_to_check = ['please', 'thanks', 'dusa', 'training', 'utrain', 'ldrd', 'code', 'dc', 'author']\n",
    "wtck = {}\n",
    "wtfidf = []\n",
    "for i in range(0, len(feature_names_mth)):\n",
    "    if feature_names_mth[i] in words_to_check:\n",
    "        wtck[feature_names_mth[i]] = i\n",
    "for mh in range(0, resp_m.shape[0]):\n",
    "    month_data = dense_month[mh].tolist()[0]\n",
    "    tot_data = len(month_data)\n",
    "    score_month = [x for x in zip(range(0, tot_data), month_data) if x[1] > 0]\n",
    "    tfidfs = []\n",
    "    for wtc in words_to_check:\n",
    "        score = -1\n",
    "        for stuff in score_month:\n",
    "            if stuff[0] == wtck[wtc]:\n",
    "                score = stuff[1]\n",
    "                break\n",
    "        tfidfs.append(wtc + \":\" + str(score))\n",
    "    \n",
    "    wtfidf.append(\";\".join(tfidfs))\n",
    "dfmprocessed['wordstfidf'] = wtfidf\n",
    "dfmprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfmprocessed[['wordstfidf']].to_csv(\"./top20_select_tfidf.txt\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
